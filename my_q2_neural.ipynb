{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# from q1_softmax import softmax\n",
    "# from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "# from q2_gradcheck import gradcheck_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\" \n",
    "    Forward and backward propagation for a two-layer sigmoidal network \n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using \n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "    \n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,\n",
    "        dimensions), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def your_sanity_checks(): \n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_neural.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print \"Running your sanity checks...\"\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape,W1.shape, b1.shape, W2.shape, b2.shape\n",
      "(20L, 10L) (10L, 5L) (1L, 5L) (5L, 10L) (1L, 10L)\n"
     ]
    }
   ],
   "source": [
    "N = 20 # Number of Observations\n",
    "dimensions= [10,5,10] # Dimensions of the NN\n",
    "data=np.random.randn(N,dimensions[0])\n",
    "labels=np.zeros((N,dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)]=1\n",
    "params=np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )\n",
    "#forward_backward_prop(data, labels, params, dimensions)\n",
    "#gradcheck_naive(lambda params: forward_backward_prop(data, labels, params, dimensions), params)\n",
    "'''\n",
    "Compute the forward propagation and for the cross entropy cost,\n",
    "and backward propagation for the gradients for all parameters.\n",
    "'''\n",
    "### Unpack network parameters (do not modify)\n",
    "ofs = 0\n",
    "Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "ofs += Dx * H\n",
    "b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "ofs += H\n",
    "W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "ofs += H * Dy\n",
    "b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "# Forward Propagation\n",
    "h=np.dot(data,W1)+b1\n",
    "y_pred=np.dot(h,W2)+b2\n",
    "\n",
    "# Backward Propagation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
